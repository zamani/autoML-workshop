{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2umSnto2_Fj"
      },
      "source": [
        "# Deep Neural Learning and Bayesian Optimization of Hyperparameters\n",
        "###*Optimizing the hyperparameters of a MLP network*\n",
        "### Mohammad Ali Zamani\n",
        "### Senior Machine Learning Scientist\n",
        " [zamani.ai](https://zamani.ai)\n",
        " \n",
        "Hyperopt parts taken from: https://automl.github.io/HpBandSter/build/html/index.html\n",
        "\n",
        "some pytorch related part are taken from: https://pytorch.org/tutorials/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxBKMV3Y8mFt",
        "outputId": "3e103df5-c0b7-4b14-d885-0beb25605550",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install hpbandster"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: hpbandster in /usr/local/lib/python3.8/dist-packages (0.7.4)\n",
            "Requirement already satisfied: Pyro4 in /usr/local/lib/python3.8/dist-packages (from hpbandster) (4.82)\n",
            "Requirement already satisfied: ConfigSpace in /usr/local/lib/python3.8/dist-packages (from hpbandster) (0.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from hpbandster) (1.7.3)\n",
            "Requirement already satisfied: serpent in /usr/local/lib/python3.8/dist-packages (from hpbandster) (1.41)\n",
            "Requirement already satisfied: netifaces in /usr/local/lib/python3.8/dist-packages (from hpbandster) (0.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from hpbandster) (1.21.6)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.8/dist-packages (from hpbandster) (0.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from ConfigSpace->hpbandster) (4.4.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.8/dist-packages (from ConfigSpace->hpbandster) (0.29.32)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.8/dist-packages (from ConfigSpace->hpbandster) (3.0.9)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.8/dist-packages (from statsmodels->hpbandster) (0.5.3)\n",
            "Requirement already satisfied: pandas>=0.21 in /usr/local/lib/python3.8/dist-packages (from statsmodels->hpbandster) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.21->statsmodels->hpbandster) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.21->statsmodels->hpbandster) (2022.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from patsy>=0.5->statsmodels->hpbandster) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwHGvAib5YvW"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(100)\n",
        "\n",
        "# for pytorch\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from typing import NamedTuple\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "import copy\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "import os\n",
        "import pickle\n",
        "import argparse\n",
        "import time\n",
        "\n",
        "# for hyperparameter optimization\n",
        "import ConfigSpace as CS\n",
        "import ConfigSpace.hyperparameters as CSH\n",
        "from hpbandster.core.worker import Worker\n",
        "import hpbandster.core.nameserver as hpns\n",
        "import hpbandster.core.result as hpres\n",
        "from hpbandster.optimizers import BOHB"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PifpfwN25cN"
      },
      "source": [
        "def twospirals(n_points, difficulty, noise=1.):\n",
        "    \"\"\"\n",
        "     Returns the two spirals dataset.\n",
        "    \"\"\"\n",
        "    n = np.sqrt(np.random.rand(n_points,1)) * difficulty * (2*np.pi)/360\n",
        "    d1x = -np.cos(n)*n + np.random.rand(n_points,1) * noise\n",
        "    d1y = np.sin(n)*n + np.random.rand(n_points,1) * noise\n",
        "    return (np.vstack((np.hstack((d1x,d1y)),np.hstack((-d1x,-d1y)))), \n",
        "            np.hstack((np.zeros(n_points),np.ones(n_points))))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfhvqEj1Rymr"
      },
      "source": [
        "class SprialDataSet(Dataset):\n",
        "    \"\"\"\n",
        "    Loading the dataset\n",
        "    \"\"\"\n",
        "    def __init__(self, n_points, difficulty):\n",
        "        super(SprialDataSet, self).__init__()\n",
        "        X, Y = twospirals(n_points, difficulty)\n",
        "        self.set_len = len(X)\n",
        "        X = torch.from_numpy(X)\n",
        "        self.input = X.type(torch.float32)\n",
        "        \n",
        "        Y = torch.from_numpy(Y)\n",
        "        Y = Y.unsqueeze(1)\n",
        "        self.label = Y.type(torch.float32)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.set_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input[idx], self.label[idx]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V4NCOl0XBKm"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "class Optimization():\n",
        "    def __init__(self, args, loss,  train_loader, val_loader, test_loader):\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.test_loader = test_loader\n",
        "        device = args.device\n",
        "\n",
        "        self.model = MLP(args).to(device)\n",
        "        \n",
        "        print(\"number of trainable parameter = \", count_parameters(self.model))\n",
        "        \n",
        "        if args.optimizer == 'Adam':\n",
        "            self.optimizer = optim.Adam(self.model.parameters(), lr=args.rate)\n",
        "        elif args.optimizer == 'SGD':\n",
        "            self.optimizer = torch.optim.SGD(self.model.parameters(), lr=args.rate, momentum=args.sgd_momentum)\n",
        "\n",
        "        self.scheduler = StepLR(self.optimizer, step_size=args.lr_decay_step)\n",
        "\n",
        "        self.loss = loss\n",
        "        self.device = device\n",
        "\n",
        "    def train(self):\n",
        "        batch_counter = 0.0\n",
        "        total_loss = 0.0\n",
        "        self.model.train()\n",
        "        for iter, data in enumerate(self.train_loader):\n",
        "            \n",
        "            inputs, labels = data \n",
        "\n",
        "            inputs = inputs.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "\n",
        "            self.model.zero_grad()\n",
        "            outputs = self.model(inputs)\n",
        "            loss = self.loss(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            self.optimizer.step()\n",
        "\n",
        "            batch_counter += 1\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "        loss_value = total_loss / batch_counter\n",
        "        return loss_value\n",
        "\n",
        "    def val_eval(self):\n",
        "        batch_counter = 0.0\n",
        "        total_loss = 0.0\n",
        "        self.model.eval()\n",
        "        for iter, data in enumerate(self.val_loader):\n",
        "            inputs, labels = data\n",
        "            \n",
        "            inputs = inputs.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "            \n",
        "            # for evaluating the network, we disable the gradient calculation with the no_grad function\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.loss(outputs, labels)\n",
        "\n",
        "            batch_counter += 1\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        loss_value = total_loss / batch_counter\n",
        "        return loss_value\n",
        "\n",
        "    def test_eval(self, graph=False):\n",
        "        batch_counter = 0.0\n",
        "        total_loss = 0.0\n",
        "        self.model.eval()\n",
        "\n",
        "        for iter, data in enumerate(self.test_loader):\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "           \n",
        "            # for evaluating the network, we disable the gradient calculation with the no_grad function\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(inputs)\n",
        "                loss = torch.mean((torch.sign(labels - 0.5) * torch.sign(outputs) > 0).type(torch.float32).to(self.device))\n",
        "\n",
        "            batch_counter += 1\n",
        "            total_loss += loss\n",
        "            \n",
        "           \n",
        "        loss_value = total_loss / batch_counter\n",
        "        return loss_value.item()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWAwkGzSUuSs"
      },
      "source": [
        "def main(args, train_loader, val_loader, test_loader):\n",
        "    device = torch.device(args.device)\n",
        "    best_val_error = np.inf\n",
        "\n",
        "    if args.loss == 'BCE':\n",
        "        loss_function = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "    elif args.loss == 'NLL':\n",
        "        loss_function = nn.NLLLoss(reduction='mean')\n",
        "\n",
        "    optimization = Optimization(args, loss_function, train_loader, val_loader, test_loader)\n",
        "\n",
        "    train_loss_records = []\n",
        "    val_loss_records = []\n",
        "    test_loss_records = []\n",
        "\n",
        "    print(\"loading training, val and test set completed!\")\n",
        "    mistake_counter = 0  # mistakes counter for validation loss\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        train_loss = optimization.train()\n",
        "        train_loss_records.append(train_loss)\n",
        "        optimization.scheduler.step()\n",
        "\n",
        "        val_loss = optimization.val_eval()\n",
        "        val_loss_records.append(val_loss)\n",
        "\n",
        "        test_loss = optimization.test_eval()\n",
        "        test_loss_records.append(test_loss)\n",
        "\n",
        "        if epoch > 1:\n",
        "            if val_loss_records[-1] > val_loss_records[-2]:\n",
        "                mistake_counter += 1\n",
        "\n",
        "        if val_loss < best_val_error:\n",
        "            best_results = {\n",
        "                'epoch': epoch + 1,\n",
        "                'state_dict': copy.deepcopy(optimization.model.state_dict()),\n",
        "                'model': optimization.model,\n",
        "                'best_val_error': val_loss,\n",
        "                'best_test_error': test_loss,\n",
        "                'optimizer': copy.deepcopy(optimization.optimizer),\n",
        "                'args': args\n",
        "            }\n",
        "            best_val_error = val_loss\n",
        "        print(\n",
        "            '[Epoch: %3d/%3d] LR: %0.8f  Train loss: %.4f,  Val loss: %.4f,  Test Acc: %.4f'\n",
        "            % (epoch + 1, args.epochs, optimization.scheduler.get_lr()[0], train_loss_records[epoch], val_loss_records[epoch],\n",
        "               test_loss_records[epoch]))\n",
        "        \n",
        "        if mistake_counter >= args.tol or epoch == args.epochs - 1:\n",
        "            print('Training is terminated. final epoch or validation loss has increased')\n",
        "            break\n",
        "    return test_loss, val_loss"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqwyYF5ajNqc"
      },
      "source": [
        "class Args(NamedTuple):\n",
        "    rate: float  # learning rate\n",
        "    lr_decay_step: int  # learning rate decay\n",
        "    batch_size: int  # minibatch size\n",
        "    epochs: int  # maximum training epochs\n",
        "    tol: int  # tolerance for the validation error increment\n",
        "    device: str  # cuda or cpu\n",
        "\n",
        "    loss: str  # loss function     \n",
        "    optimizer: str # optimizer method\n",
        "    sgd_momentum: float #\n",
        "\n",
        "    dropout: float  # the probability for dropout \n",
        "    fc1: int # 1st hidden layer's units\n",
        "    fc2: int\n",
        "    fc3: int\n",
        "    num_layers: int\n",
        "    # TODO1: add more layers or parameters if necessary"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgXNN13CRJ0O"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(MLP, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=args.dropout)\n",
        "        self.args = args\n",
        "\n",
        "        self.fc1 = nn.Linear(2, args.fc1)\n",
        "        out = args.fc1\n",
        "\n",
        "        if args.num_layers >= 2:\n",
        "            self.fc2 = nn.Linear(args.fc1, args.fc2)\n",
        "            out = args.fc2\n",
        "\n",
        "        elif args.num_layers == 3:\n",
        "            self.fc3 = nn.Linear(args.fc2, args.fc3)\n",
        "            out = args.fc3\n",
        "\n",
        "        self.fc_out = nn.Linear(out, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        \n",
        "        if self.args.num_layers >= 2:\n",
        "          x = self.dropout(torch.relu(self.fc2(x)))\n",
        "\n",
        "        elif self.args.num_layers >= 3:\n",
        "          x = self.dropout(torch.relu(self.fc3(x)))\n",
        "        \n",
        "        out = self.fc_out(x)\n",
        "        # TODO5 construct the network here\n",
        "        return out"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vwkyj09W49X5"
      },
      "source": [
        "class PyTorchWorker(Worker):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        \n",
        "        # initialize the arguments for loading the data\n",
        "        difficulty = 500\n",
        "        batch_size = 32\n",
        "\n",
        "        training_set = SprialDataSet(64, difficulty)\n",
        "        val_set = SprialDataSet(32, difficulty)\n",
        "        test_set = SprialDataSet(512, difficulty)\n",
        "\n",
        "        self.train_loader = DataLoader(training_set, batch_size=batch_size, num_workers=8, shuffle=True, drop_last=True)\n",
        "        self.val_loader = DataLoader(val_set, batch_size=batch_size, num_workers=8, shuffle=False, drop_last=True)\n",
        "        self.test_loader = DataLoader(test_set, batch_size=batch_size, num_workers=8, shuffle=False, drop_last=True)\n",
        "\n",
        "    def compute(self, config, budget, working_directory, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        testing the configuration\n",
        "        \"\"\"\n",
        "        print(\"Selected HyperParameters to test: \")\n",
        "        print(config)\n",
        "        new_args = Args(\n",
        "                        rate=config['lr'],\n",
        "                        lr_decay_step=30,\n",
        "                        batch_size=32,\n",
        "                        epochs=int(budget),\n",
        "                        tol=100,\n",
        "                        loss='BCE',\n",
        "                        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "\n",
        "                        optimizer=config['optimizer'],\n",
        "                        sgd_momentum= config['sgd_momentum'] if 'sgd_momentum' in config else None,\n",
        "                        dropout= config['dropout'],\n",
        "                        fc1=config['fc1'], # \n",
        "                        fc2=config['fc2'] if 'fc2' in config else None,\n",
        "                        fc3=config['fc3'] if 'fc3' in config else None,\n",
        "                        num_layers=config['num_layers']\n",
        "                        # TODO2 add the additional parameters from Args class here\n",
        "                        )\n",
        "        \n",
        "        test_loss, val_loss = main(new_args, self.train_loader, self.val_loader, self.test_loader)\n",
        "        return ({\n",
        "            'loss': val_loss,  # remember: HpBandSter always minimizes!\n",
        "            'info': {'test accuracy': test_loss,\n",
        "                     }\n",
        "        })\n",
        "\n",
        "    \n",
        "    @staticmethod\n",
        "    def get_configspace():\n",
        "        \"\"\"\n",
        "            It builds the configuration space with the needed hyperparameters\n",
        "            :return: ConfigurationsSpace-Object\n",
        "            \"\"\"\n",
        "        cs = CS.ConfigurationSpace()\n",
        "\n",
        "        # TODO3: add a proper condition here\n",
        "        # Type 1 condition: float\n",
        "        lr = CSH.UniformFloatHyperparameter('lr', lower=1e-4, upper=1e-1, default_value='1e-2', log=True)\n",
        "        \n",
        "        # Type 2 condition: categorical \n",
        "        # For demonstration purposes, we add different optimizers as categorical hyperparameters.\n",
        "        # To show how to use conditional hyperparameters with ConfigSpace, we'll add the optimizers 'Adam' and 'SGD'.\n",
        "        # SGD has a different parameter 'momentum'.\n",
        "        optimizer = CSH.CategoricalHyperparameter('optimizer', ['Adam', 'SGD'])\n",
        "\n",
        "        sgd_momentum = CSH.UniformFloatHyperparameter('sgd_momentum', lower=0.0, upper=0.99, default_value=0.9, log=False)\n",
        "\n",
        "        cs.add_hyperparameters([lr, optimizer, sgd_momentum])\n",
        "        #cs.add_hyperparameters([lr])\n",
        "\n",
        "        # Type 3 condition: conditional\n",
        "        # The hyperparameter sgd_momentum will be used,if the configuration\n",
        "        # contains 'SGD' as optimizer.\n",
        "        cond = CS.EqualsCondition(sgd_momentum, optimizer, 'SGD')\n",
        "        cs.add_condition(cond)\n",
        "\n",
        "        # Type 4 condition: Integer\n",
        "        fc1 = CSH.UniformIntegerHyperparameter('fc1', lower=2, upper=20, default_value=10, log=False)\n",
        "        fc2 = CSH.UniformIntegerHyperparameter('fc2', lower=2, upper=20, default_value=10, log=False)\n",
        "        fc3 = CSH.UniformIntegerHyperparameter('fc3', lower=2, upper=20, default_value=10, log=False)\n",
        "        num_layers = CSH.UniformIntegerHyperparameter('num_layers', lower=1, upper=3, default_value=2, log=False)\n",
        "\n",
        "        cs.add_hyperparameters([fc1, fc2, fc3, num_layers])\n",
        "\n",
        "        cond_fc2 = CS.GreaterThanCondition(fc2, num_layers, 1)\n",
        "        cond_fc3 = CS.GreaterThanCondition(fc3, num_layers, 2)\n",
        "        cs.add_condition(cond_fc2)\n",
        "        cs.add_condition(cond_fc3)\n",
        "\n",
        "        dropout = CSH.UniformFloatHyperparameter('dropout', lower=0, upper=0.9, default_value='0', log=False)\n",
        "        cs.add_hyperparameters([dropout])\n",
        "        return cs"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgj2uEAw4xOU",
        "outputId": "9121eb5e-8340-43f9-8216-854108730c02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# TO record a backup from the old hyperparamter searches\n",
        "def backup_jsons(curDir):\n",
        "    for fname in ('results', 'configs'):\n",
        "        if os.path.exists(curDir + os.path.sep + fname + '.json'):\n",
        "            counter = 1\n",
        "            while os.path.exists(curDir + os.path.sep + fname + '_' + str(counter) + '.json'):\n",
        "                counter += 1\n",
        "            os.rename(curDir + os.path.sep + fname + '.json', curDir + os.path.sep + fname + '_' + str(counter) + '.json')\n",
        "\n",
        "\n",
        "args_min_budget = 1 # Minimum number of epochs for training.\n",
        "args_max_budget = 27 # Maximum number of epochs for training.\n",
        "args_n_iterations = 3 # Number of iterations performed by the optimizer\n",
        "args_worker = False # Flag to turn this into a worker process\n",
        "args_run_id = '' # A unique run id for this optimization run. An easy option is to use the job id of the clusters scheduler.\n",
        "args_nic_name = 'lo' # Which network interface to use for communication.\n",
        "args_shared_directory = '.' # A directory that is accessible for all processes, e.g. a NFS share.\n",
        "args_eta = 3 # eta\n",
        "\n",
        "# Every process has to lookup the hostname\n",
        "host = hpns.nic_name_to_host(args_nic_name)\n",
        "\n",
        "\n",
        "if args_worker:\n",
        "    import time\n",
        "    time.sleep(1)   # short artificial delay to make sure the nameserver is already running\n",
        "    w = PyTorchWorker(run_id=args_run_id, host=host, timeout=120)\n",
        "    w.load_nameserver_credentials(working_directory=args_shared_directory)\n",
        "    w.run(background=False)\n",
        "    exit(0)\n",
        "\n",
        "\n",
        "# This example shows how to log live results. This is most useful\n",
        "# for really long runs, where intermediate results could already be\n",
        "# interesting. The core.result submodule contains the functionality to\n",
        "# read the two generated files (results.json and configs.json) and\n",
        "# create a Result object.\n",
        "\n",
        "#backup_jsons(args_shared_directory)\n",
        "result_logger = hpres.json_result_logger(directory=args_shared_directory, overwrite=True)\n",
        "\n",
        "# Start a nameserver:\n",
        "NS = hpns.NameServer(run_id=args_run_id, host=host, port=0, working_directory=args_shared_directory)\n",
        "ns_host, ns_port = NS.start()\n",
        "\n",
        "# Start local worker\n",
        "w = PyTorchWorker(run_id=args_run_id, host=host, nameserver=ns_host, nameserver_port=ns_port, timeout=120)\n",
        "w.run(background=True)\n",
        "\n",
        "# Run an optimizer\n",
        "bohb = BOHB(  configspace = PyTorchWorker.get_configspace(),\n",
        "                        run_id = args_run_id,\n",
        "                        eta = args_eta,\n",
        "                        host=host,\n",
        "                        nameserver=ns_host,\n",
        "                        nameserver_port=ns_port,\n",
        "                        result_logger=result_logger,\n",
        "                        min_budget=args_min_budget, \n",
        "                        max_budget=args_max_budget,\n",
        "                        \n",
        "                        # in case of expecting better result change the following \n",
        "                        num_samples = 32, # 1st decrease\n",
        "                        top_n_percent=25, # 2nd increase \n",
        "                        bandwidth_factor=10, # 3rd increase or/and\n",
        "                        min_bandwidth=1e-3, # 3rd increase\n",
        "            \n",
        "               )\n",
        "res = bohb.run(n_iterations=args_n_iterations)\n",
        "\n",
        "# store results\n",
        "with open(os.path.join(args_shared_directory, 'results.pkl'), 'wb') as fh:\n",
        "    pickle.dump(res, fh)\n",
        "\n",
        "# shutdown\n",
        "bohb.shutdown(shutdown_workers=True)\n",
        "NS.shutdown()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.7956881371660035, 'fc1': 8, 'lr': 0.0012322847944916684, 'num_layers': 2, 'optimizer': 'Adam', 'fc2': 6}\n",
            "number of trainable parameter =  85\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  1] LR: 0.00123228  Train loss: 0.9729,  Val loss: 0.7942,  Test Acc: 0.4775\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.6903817768031177, 'fc1': 11, 'lr': 0.06989777058656244, 'num_layers': 2, 'optimizer': 'SGD', 'fc2': 20, 'sgd_momentum': 0.6942963079113548}\n",
            "number of trainable parameter =  294\n",
            "loading training, val and test set completed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:381: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch:   1/  1] LR: 0.06989777  Train loss: 0.6842,  Val loss: 0.5851,  Test Acc: 0.6562\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.32646064460895496, 'fc1': 14, 'lr': 0.022810600297757926, 'num_layers': 2, 'optimizer': 'SGD', 'fc2': 9, 'sgd_momentum': 0.06182896619664594}\n",
            "number of trainable parameter =  187\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  1] LR: 0.02281060  Train loss: 0.7308,  Val loss: 0.7340,  Test Acc: 0.4893\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.4154182385693939, 'fc1': 9, 'lr': 0.00320140675285429, 'num_layers': 3, 'optimizer': 'SGD', 'fc2': 18, 'fc3': 4, 'sgd_momentum': 0.5445802583971601}\n",
            "number of trainable parameter =  226\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  1] LR: 0.00320141  Train loss: 0.8211,  Val loss: 0.7824,  Test Acc: 0.4043\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.264909224833196, 'fc1': 2, 'lr': 0.017096341302622296, 'num_layers': 3, 'optimizer': 'Adam', 'fc2': 17, 'fc3': 9}\n",
            "number of trainable parameter =  75\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  1] LR: 0.01709634  Train loss: 0.7033,  Val loss: 0.7071,  Test Acc: 0.4932\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.0009409261907663447, 'fc1': 15, 'lr': 0.00030327003242177356, 'num_layers': 2, 'optimizer': 'Adam', 'fc2': 6}\n",
            "number of trainable parameter =  148\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  1] LR: 0.00030327  Train loss: 0.6825,  Val loss: 0.6677,  Test Acc: 0.5000\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.8788299795683447, 'fc1': 3, 'lr': 0.00016222569296095806, 'num_layers': 2, 'optimizer': 'Adam', 'fc2': 6}\n",
            "number of trainable parameter =  40\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  1] LR: 0.00016223  Train loss: 0.8620,  Val loss: 0.7730,  Test Acc: 0.4990\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.625205619919402, 'fc1': 20, 'lr': 0.0018658768478543863, 'num_layers': 3, 'optimizer': 'Adam', 'fc2': 19, 'fc3': 17}\n",
            "number of trainable parameter =  479\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  1] LR: 0.00186588  Train loss: 0.7912,  Val loss: 0.7736,  Test Acc: 0.4219\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.3678042454173934, 'fc1': 8, 'lr': 0.0014075617015058257, 'num_layers': 3, 'optimizer': 'SGD', 'fc2': 4, 'fc3': 16, 'sgd_momentum': 0.30837571963273835}\n",
            "number of trainable parameter =  65\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  1] LR: 0.00140756  Train loss: 0.7100,  Val loss: 0.7069,  Test Acc: 0.5654\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.7748589594884724, 'fc1': 10, 'lr': 0.07303960792430558, 'num_layers': 1, 'optimizer': 'SGD', 'sgd_momentum': 0.7239311283435298}\n",
            "number of trainable parameter =  41\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  1] LR: 0.07303961  Train loss: 0.6453,  Val loss: 0.6026,  Test Acc: 0.5938\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.3138073858221204, 'fc1': 6, 'lr': 0.018810597137794635, 'num_layers': 2, 'optimizer': 'SGD', 'fc2': 8, 'sgd_momentum': 0.37770891778637}\n",
            "number of trainable parameter =  83\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  1] LR: 0.01881060  Train loss: 0.7487,  Val loss: 0.6931,  Test Acc: 0.5771\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.8939200683695802, 'fc1': 12, 'lr': 0.0032898165401257272, 'num_layers': 2, 'optimizer': 'Adam', 'fc2': 17}\n",
            "number of trainable parameter =  275\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  1] LR: 0.00328982  Train loss: 0.7952,  Val loss: 0.6949,  Test Acc: 0.5449\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.6066109452984831, 'fc1': 17, 'lr': 0.002576366583222281, 'num_layers': 3, 'optimizer': 'SGD', 'fc2': 18, 'fc3': 15, 'sgd_momentum': 0.634626440324903}\n",
            "number of trainable parameter =  394\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  1] LR: 0.00257637  Train loss: 0.7330,  Val loss: 0.7190,  Test Acc: 0.5000\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.4558373632593114, 'fc1': 9, 'lr': 0.050194305125014695, 'num_layers': 3, 'optimizer': 'Adam', 'fc2': 18, 'fc3': 3}\n",
            "number of trainable parameter =  226\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  1] LR: 0.05019431  Train loss: 0.7005,  Val loss: 0.6408,  Test Acc: 0.5293\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.2911385505232891, 'fc1': 13, 'lr': 0.016031866259628425, 'num_layers': 2, 'optimizer': 'SGD', 'fc2': 10, 'sgd_momentum': 0.08182030042440057}\n",
            "number of trainable parameter =  190\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  1] LR: 0.01603187  Train loss: 0.7156,  Val loss: 0.6980,  Test Acc: 0.4629\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.6648809070967873, 'fc1': 17, 'lr': 0.013469000214153357, 'num_layers': 1, 'optimizer': 'Adam'}\n",
            "number of trainable parameter =  69\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  1] LR: 0.01346900  Train loss: 0.8931,  Val loss: 0.7350,  Test Acc: 0.4443\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.09123099701586451, 'fc1': 4, 'lr': 0.0033832799710452823, 'num_layers': 1, 'optimizer': 'Adam'}\n",
            "number of trainable parameter =  17\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  1] LR: 0.00338328  Train loss: 0.8621,  Val loss: 0.8903,  Test Acc: 0.3770\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.3143330358229714, 'fc1': 11, 'lr': 0.00028770815728209996, 'num_layers': 2, 'optimizer': 'SGD', 'fc2': 2, 'sgd_momentum': 0.6369909681097302}\n",
            "number of trainable parameter =  60\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  1] LR: 0.00028771  Train loss: 0.7224,  Val loss: 0.7302,  Test Acc: 0.5000\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.7041032904926356, 'fc1': 9, 'lr': 0.0013437797832444804, 'num_layers': 2, 'optimizer': 'SGD', 'fc2': 16, 'sgd_momentum': 0.7062695865326974}\n",
            "number of trainable parameter =  204\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  1] LR: 0.00134378  Train loss: 0.7383,  Val loss: 0.7150,  Test Acc: 0.5000\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.028095983174232033, 'fc1': 5, 'lr': 0.0018586194334476544, 'num_layers': 3, 'optimizer': 'Adam', 'fc2': 16, 'fc3': 5}\n",
            "number of trainable parameter =  128\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  1] LR: 0.00185862  Train loss: 0.6732,  Val loss: 0.6493,  Test Acc: 0.6240\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.6328311171044985, 'fc1': 6, 'lr': 0.0006696931198741266, 'num_layers': 3, 'optimizer': 'Adam', 'fc2': 3, 'fc3': 17}\n",
            "number of trainable parameter =  43\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  1] LR: 0.00066969  Train loss: 0.7860,  Val loss: 0.7837,  Test Acc: 0.3613\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.07235053431514404, 'fc1': 7, 'lr': 0.06784616288434882, 'num_layers': 2, 'optimizer': 'SGD', 'fc2': 4, 'sgd_momentum': 0.9873180047005893}\n",
            "number of trainable parameter =  58\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  1] LR: 0.06784616  Train loss: 0.7441,  Val loss: 0.6937,  Test Acc: 0.4697\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.8713150712352131, 'fc1': 10, 'lr': 0.05653250529903754, 'num_layers': 3, 'optimizer': 'SGD', 'fc2': 17, 'fc3': 2, 'sgd_momentum': 0.8662403765495755}\n",
            "number of trainable parameter =  235\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  1] LR: 0.05653251  Train loss: 0.9012,  Val loss: 0.7179,  Test Acc: 0.5088\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.3587845980903494, 'fc1': 7, 'lr': 0.06127277375568422, 'num_layers': 1, 'optimizer': 'Adam'}\n",
            "number of trainable parameter =  29\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  1] LR: 0.06127277  Train loss: 0.8759,  Val loss: 0.6291,  Test Acc: 0.5625\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.3796198467874985, 'fc1': 14, 'lr': 0.0528093187042967, 'num_layers': 1, 'optimizer': 'SGD', 'sgd_momentum': 0.6992136081525866}\n",
            "number of trainable parameter =  57\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  1] LR: 0.05280932  Train loss: 0.6724,  Val loss: 0.6300,  Test Acc: 0.6035\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.685142456382093, 'fc1': 13, 'lr': 0.013847596188593077, 'num_layers': 3, 'optimizer': 'SGD', 'fc2': 13, 'fc3': 3, 'sgd_momentum': 0.6788953192279308}\n",
            "number of trainable parameter =  235\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  1] LR: 0.01384760  Train loss: 0.7814,  Val loss: 0.7129,  Test Acc: 0.5000\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.0961496043032314, 'fc1': 5, 'lr': 0.02875279285264244, 'num_layers': 3, 'optimizer': 'Adam', 'fc2': 4, 'fc3': 4}\n",
            "number of trainable parameter =  44\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  1] LR: 0.02875279  Train loss: 0.6441,  Val loss: 0.6303,  Test Acc: 0.6182\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.6903817768031177, 'fc1': 11, 'lr': 0.06989777058656244, 'num_layers': 2, 'optimizer': 'SGD', 'fc2': 20, 'sgd_momentum': 0.6942963079113548}\n",
            "number of trainable parameter =  294\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  3] LR: 0.06989777  Train loss: 0.7334,  Val loss: 0.6202,  Test Acc: 0.6211\n",
            "[Epoch:   2/  3] LR: 0.06989777  Train loss: 0.6880,  Val loss: 0.6038,  Test Acc: 0.6318\n",
            "[Epoch:   3/  3] LR: 0.06989777  Train loss: 0.6651,  Val loss: 0.5921,  Test Acc: 0.6484\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.0009409261907663447, 'fc1': 15, 'lr': 0.00030327003242177356, 'num_layers': 2, 'optimizer': 'Adam', 'fc2': 6}\n",
            "number of trainable parameter =  148\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  3] LR: 0.00030327  Train loss: 0.7368,  Val loss: 0.7372,  Test Acc: 0.5000\n",
            "[Epoch:   2/  3] LR: 0.00030327  Train loss: 0.7347,  Val loss: 0.7345,  Test Acc: 0.5000\n",
            "[Epoch:   3/  3] LR: 0.00030327  Train loss: 0.7321,  Val loss: 0.7320,  Test Acc: 0.5000\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.7748589594884724, 'fc1': 10, 'lr': 0.07303960792430558, 'num_layers': 1, 'optimizer': 'SGD', 'sgd_momentum': 0.7239311283435298}\n",
            "number of trainable parameter =  41\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  3] LR: 0.07303961  Train loss: 0.7334,  Val loss: 0.5828,  Test Acc: 0.6396\n",
            "[Epoch:   2/  3] LR: 0.07303961  Train loss: 0.6231,  Val loss: 0.5823,  Test Acc: 0.6299\n",
            "[Epoch:   3/  3] LR: 0.07303961  Train loss: 0.6292,  Val loss: 0.5612,  Test Acc: 0.6426\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.3138073858221204, 'fc1': 6, 'lr': 0.018810597137794635, 'num_layers': 2, 'optimizer': 'SGD', 'fc2': 8, 'sgd_momentum': 0.37770891778637}\n",
            "number of trainable parameter =  83\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  3] LR: 0.01881060  Train loss: 0.7570,  Val loss: 0.6946,  Test Acc: 0.4951\n",
            "[Epoch:   2/  3] LR: 0.01881060  Train loss: 0.7384,  Val loss: 0.6759,  Test Acc: 0.4941\n",
            "[Epoch:   3/  3] LR: 0.01881060  Train loss: 0.7363,  Val loss: 0.6616,  Test Acc: 0.5293\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.4558373632593114, 'fc1': 9, 'lr': 0.050194305125014695, 'num_layers': 3, 'optimizer': 'Adam', 'fc2': 18, 'fc3': 3}\n",
            "number of trainable parameter =  226\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  3] LR: 0.05019431  Train loss: 0.6589,  Val loss: 0.5600,  Test Acc: 0.6621\n",
            "[Epoch:   2/  3] LR: 0.05019431  Train loss: 0.6563,  Val loss: 0.5771,  Test Acc: 0.6367\n",
            "[Epoch:   3/  3] LR: 0.05019431  Train loss: 0.6719,  Val loss: 0.5812,  Test Acc: 0.6074\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.028095983174232033, 'fc1': 5, 'lr': 0.0018586194334476544, 'num_layers': 3, 'optimizer': 'Adam', 'fc2': 16, 'fc3': 5}\n",
            "number of trainable parameter =  128\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  3] LR: 0.00185862  Train loss: 0.7665,  Val loss: 0.7615,  Test Acc: 0.5000\n",
            "[Epoch:   2/  3] LR: 0.00185862  Train loss: 0.7481,  Val loss: 0.7423,  Test Acc: 0.5000\n",
            "[Epoch:   3/  3] LR: 0.00185862  Train loss: 0.7331,  Val loss: 0.7252,  Test Acc: 0.5000\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.3587845980903494, 'fc1': 7, 'lr': 0.06127277375568422, 'num_layers': 1, 'optimizer': 'Adam'}\n",
            "number of trainable parameter =  29\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  3] LR: 0.06127277  Train loss: 0.7519,  Val loss: 0.5655,  Test Acc: 0.6660\n",
            "[Epoch:   2/  3] LR: 0.06127277  Train loss: 0.6359,  Val loss: 0.5667,  Test Acc: 0.6123\n",
            "[Epoch:   3/  3] LR: 0.06127277  Train loss: 0.6183,  Val loss: 0.5523,  Test Acc: 0.6152\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.3796198467874985, 'fc1': 14, 'lr': 0.0528093187042967, 'num_layers': 1, 'optimizer': 'SGD', 'sgd_momentum': 0.6992136081525866}\n",
            "number of trainable parameter =  57\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  3] LR: 0.05280932  Train loss: 0.6984,  Val loss: 0.6593,  Test Acc: 0.5723\n",
            "[Epoch:   2/  3] LR: 0.05280932  Train loss: 0.6222,  Val loss: 0.5663,  Test Acc: 0.6201\n",
            "[Epoch:   3/  3] LR: 0.05280932  Train loss: 0.6170,  Val loss: 0.5363,  Test Acc: 0.6465\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.0961496043032314, 'fc1': 5, 'lr': 0.02875279285264244, 'num_layers': 3, 'optimizer': 'Adam', 'fc2': 4, 'fc3': 4}\n",
            "number of trainable parameter =  44\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  3] LR: 0.02875279  Train loss: 0.7370,  Val loss: 0.6860,  Test Acc: 0.5000\n",
            "[Epoch:   2/  3] LR: 0.02875279  Train loss: 0.6843,  Val loss: 0.6763,  Test Acc: 0.6367\n",
            "[Epoch:   3/  3] LR: 0.02875279  Train loss: 0.6657,  Val loss: 0.6681,  Test Acc: 0.5635\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.7748589594884724, 'fc1': 10, 'lr': 0.07303960792430558, 'num_layers': 1, 'optimizer': 'SGD', 'sgd_momentum': 0.7239311283435298}\n",
            "number of trainable parameter =  41\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  9] LR: 0.07303961  Train loss: 0.6829,  Val loss: 0.5640,  Test Acc: 0.6377\n",
            "[Epoch:   2/  9] LR: 0.07303961  Train loss: 0.6578,  Val loss: 0.5788,  Test Acc: 0.6338\n",
            "[Epoch:   3/  9] LR: 0.07303961  Train loss: 0.6230,  Val loss: 0.5801,  Test Acc: 0.6416\n",
            "[Epoch:   4/  9] LR: 0.07303961  Train loss: 0.6331,  Val loss: 0.5717,  Test Acc: 0.6445\n",
            "[Epoch:   5/  9] LR: 0.07303961  Train loss: 0.6199,  Val loss: 0.5659,  Test Acc: 0.6514\n",
            "[Epoch:   6/  9] LR: 0.07303961  Train loss: 0.6185,  Val loss: 0.5583,  Test Acc: 0.6533\n",
            "[Epoch:   7/  9] LR: 0.07303961  Train loss: 0.6167,  Val loss: 0.5723,  Test Acc: 0.6426\n",
            "[Epoch:   8/  9] LR: 0.07303961  Train loss: 0.6160,  Val loss: 0.5590,  Test Acc: 0.6523\n",
            "[Epoch:   9/  9] LR: 0.07303961  Train loss: 0.6157,  Val loss: 0.5657,  Test Acc: 0.6484\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.3587845980903494, 'fc1': 7, 'lr': 0.06127277375568422, 'num_layers': 1, 'optimizer': 'Adam'}\n",
            "number of trainable parameter =  29\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  9] LR: 0.06127277  Train loss: 0.7517,  Val loss: 0.5774,  Test Acc: 0.6367\n",
            "[Epoch:   2/  9] LR: 0.06127277  Train loss: 0.6607,  Val loss: 0.6027,  Test Acc: 0.6279\n",
            "[Epoch:   3/  9] LR: 0.06127277  Train loss: 0.6373,  Val loss: 0.5557,  Test Acc: 0.6299\n",
            "[Epoch:   4/  9] LR: 0.06127277  Train loss: 0.6204,  Val loss: 0.5616,  Test Acc: 0.6279\n",
            "[Epoch:   5/  9] LR: 0.06127277  Train loss: 0.6217,  Val loss: 0.5655,  Test Acc: 0.6250\n",
            "[Epoch:   6/  9] LR: 0.06127277  Train loss: 0.6120,  Val loss: 0.5548,  Test Acc: 0.6289\n",
            "[Epoch:   7/  9] LR: 0.06127277  Train loss: 0.6033,  Val loss: 0.5470,  Test Acc: 0.6230\n",
            "[Epoch:   8/  9] LR: 0.06127277  Train loss: 0.5970,  Val loss: 0.5215,  Test Acc: 0.6504\n",
            "[Epoch:   9/  9] LR: 0.06127277  Train loss: 0.5812,  Val loss: 0.5276,  Test Acc: 0.6543\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.3796198467874985, 'fc1': 14, 'lr': 0.0528093187042967, 'num_layers': 1, 'optimizer': 'SGD', 'sgd_momentum': 0.6992136081525866}\n",
            "number of trainable parameter =  57\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  9] LR: 0.05280932  Train loss: 0.8040,  Val loss: 0.5684,  Test Acc: 0.6309\n",
            "[Epoch:   2/  9] LR: 0.05280932  Train loss: 0.6342,  Val loss: 0.5324,  Test Acc: 0.6514\n",
            "[Epoch:   3/  9] LR: 0.05280932  Train loss: 0.6226,  Val loss: 0.5407,  Test Acc: 0.6514\n",
            "[Epoch:   4/  9] LR: 0.05280932  Train loss: 0.6214,  Val loss: 0.5605,  Test Acc: 0.6367\n",
            "[Epoch:   5/  9] LR: 0.05280932  Train loss: 0.6108,  Val loss: 0.5567,  Test Acc: 0.6348\n",
            "[Epoch:   6/  9] LR: 0.05280932  Train loss: 0.6092,  Val loss: 0.5494,  Test Acc: 0.6436\n",
            "[Epoch:   7/  9] LR: 0.05280932  Train loss: 0.6171,  Val loss: 0.5448,  Test Acc: 0.6416\n",
            "[Epoch:   8/  9] LR: 0.05280932  Train loss: 0.6111,  Val loss: 0.5508,  Test Acc: 0.6377\n",
            "[Epoch:   9/  9] LR: 0.05280932  Train loss: 0.6108,  Val loss: 0.5531,  Test Acc: 0.6309\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.3587845980903494, 'fc1': 7, 'lr': 0.06127277375568422, 'num_layers': 1, 'optimizer': 'Adam'}\n",
            "number of trainable parameter =  29\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/ 27] LR: 0.06127277  Train loss: 0.7402,  Val loss: 0.6201,  Test Acc: 0.6064\n",
            "[Epoch:   2/ 27] LR: 0.06127277  Train loss: 0.6826,  Val loss: 0.6005,  Test Acc: 0.6602\n",
            "[Epoch:   3/ 27] LR: 0.06127277  Train loss: 0.6565,  Val loss: 0.6618,  Test Acc: 0.6211\n",
            "[Epoch:   4/ 27] LR: 0.06127277  Train loss: 0.6578,  Val loss: 0.6148,  Test Acc: 0.6279\n",
            "[Epoch:   5/ 27] LR: 0.06127277  Train loss: 0.6244,  Val loss: 0.5665,  Test Acc: 0.6436\n",
            "[Epoch:   6/ 27] LR: 0.06127277  Train loss: 0.6266,  Val loss: 0.5551,  Test Acc: 0.6406\n",
            "[Epoch:   7/ 27] LR: 0.06127277  Train loss: 0.6159,  Val loss: 0.5638,  Test Acc: 0.6299\n",
            "[Epoch:   8/ 27] LR: 0.06127277  Train loss: 0.6108,  Val loss: 0.5617,  Test Acc: 0.6328\n",
            "[Epoch:   9/ 27] LR: 0.06127277  Train loss: 0.6045,  Val loss: 0.5477,  Test Acc: 0.6230\n",
            "[Epoch:  10/ 27] LR: 0.06127277  Train loss: 0.5987,  Val loss: 0.5459,  Test Acc: 0.6201\n",
            "[Epoch:  11/ 27] LR: 0.06127277  Train loss: 0.5925,  Val loss: 0.5410,  Test Acc: 0.6309\n",
            "[Epoch:  12/ 27] LR: 0.06127277  Train loss: 0.5842,  Val loss: 0.5301,  Test Acc: 0.6455\n",
            "[Epoch:  13/ 27] LR: 0.06127277  Train loss: 0.5769,  Val loss: 0.5168,  Test Acc: 0.6680\n",
            "[Epoch:  14/ 27] LR: 0.06127277  Train loss: 0.5648,  Val loss: 0.5030,  Test Acc: 0.7061\n",
            "[Epoch:  15/ 27] LR: 0.06127277  Train loss: 0.5585,  Val loss: 0.4903,  Test Acc: 0.6973\n",
            "[Epoch:  16/ 27] LR: 0.06127277  Train loss: 0.5429,  Val loss: 0.4798,  Test Acc: 0.7256\n",
            "[Epoch:  17/ 27] LR: 0.06127277  Train loss: 0.5339,  Val loss: 0.4669,  Test Acc: 0.7461\n",
            "[Epoch:  18/ 27] LR: 0.06127277  Train loss: 0.5250,  Val loss: 0.4570,  Test Acc: 0.7539\n",
            "[Epoch:  19/ 27] LR: 0.06127277  Train loss: 0.5148,  Val loss: 0.4671,  Test Acc: 0.7695\n",
            "[Epoch:  20/ 27] LR: 0.06127277  Train loss: 0.4998,  Val loss: 0.4648,  Test Acc: 0.7529\n",
            "[Epoch:  21/ 27] LR: 0.06127277  Train loss: 0.4894,  Val loss: 0.4420,  Test Acc: 0.7676\n",
            "[Epoch:  22/ 27] LR: 0.06127277  Train loss: 0.4708,  Val loss: 0.4346,  Test Acc: 0.7725\n",
            "[Epoch:  23/ 27] LR: 0.06127277  Train loss: 0.4602,  Val loss: 0.4103,  Test Acc: 0.7480\n",
            "[Epoch:  24/ 27] LR: 0.06127277  Train loss: 0.4485,  Val loss: 0.4211,  Test Acc: 0.7285\n",
            "[Epoch:  25/ 27] LR: 0.06127277  Train loss: 0.4371,  Val loss: 0.4184,  Test Acc: 0.7090\n",
            "[Epoch:  26/ 27] LR: 0.06127277  Train loss: 0.4282,  Val loss: 0.4023,  Test Acc: 0.7305\n",
            "[Epoch:  27/ 27] LR: 0.06127277  Train loss: 0.4148,  Val loss: 0.3907,  Test Acc: 0.7344\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.6235731552297695, 'fc1': 2, 'lr': 0.020774732489579218, 'num_layers': 2, 'optimizer': 'SGD', 'fc2': 8, 'sgd_momentum': 0.25266179774757097}\n",
            "number of trainable parameter =  39\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  3] LR: 0.02077473  Train loss: 0.7107,  Val loss: 0.6238,  Test Acc: 0.6064\n",
            "[Epoch:   2/  3] LR: 0.02077473  Train loss: 0.7072,  Val loss: 0.6240,  Test Acc: 0.6094\n",
            "[Epoch:   3/  3] LR: 0.02077473  Train loss: 0.7344,  Val loss: 0.6253,  Test Acc: 0.6104\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.23926990426296638, 'fc1': 12, 'lr': 0.08440803961148495, 'num_layers': 1, 'optimizer': 'Adam'}\n",
            "number of trainable parameter =  49\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  3] LR: 0.08440804  Train loss: 0.7371,  Val loss: 0.5543,  Test Acc: 0.6514\n",
            "[Epoch:   2/  3] LR: 0.08440804  Train loss: 0.6449,  Val loss: 0.5782,  Test Acc: 0.6426\n",
            "[Epoch:   3/  3] LR: 0.08440804  Train loss: 0.6239,  Val loss: 0.5709,  Test Acc: 0.6250\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.4960236627496929, 'fc1': 11, 'lr': 0.04461760223753708, 'num_layers': 1, 'optimizer': 'Adam'}\n",
            "number of trainable parameter =  45\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  3] LR: 0.04461760  Train loss: 0.8747,  Val loss: 0.5805,  Test Acc: 0.6270\n",
            "[Epoch:   2/  3] LR: 0.04461760  Train loss: 0.6477,  Val loss: 0.6208,  Test Acc: 0.6367\n",
            "[Epoch:   3/  3] LR: 0.04461760  Train loss: 0.6463,  Val loss: 0.5923,  Test Acc: 0.6357\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.4924585792794713, 'fc1': 4, 'lr': 0.003831214781437969, 'num_layers': 1, 'optimizer': 'Adam'}\n",
            "number of trainable parameter =  17\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  3] LR: 0.00383121  Train loss: 0.6872,  Val loss: 0.6269,  Test Acc: 0.5898\n",
            "[Epoch:   2/  3] LR: 0.00383121  Train loss: 0.6779,  Val loss: 0.6218,  Test Acc: 0.6152\n",
            "[Epoch:   3/  3] LR: 0.00383121  Train loss: 0.6700,  Val loss: 0.6171,  Test Acc: 0.5918\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.5982229598822558, 'fc1': 4, 'lr': 0.03209149010275587, 'num_layers': 2, 'optimizer': 'SGD', 'fc2': 15, 'sgd_momentum': 0.46868162621144327}\n",
            "number of trainable parameter =  103\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  3] LR: 0.03209149  Train loss: 0.7011,  Val loss: 0.6015,  Test Acc: 0.6738\n",
            "[Epoch:   2/  3] LR: 0.03209149  Train loss: 0.6559,  Val loss: 0.6020,  Test Acc: 0.6650\n",
            "[Epoch:   3/  3] LR: 0.03209149  Train loss: 0.6653,  Val loss: 0.6043,  Test Acc: 0.6582\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.7704838709389494, 'fc1': 9, 'lr': 0.025267404740427272, 'num_layers': 1, 'optimizer': 'SGD', 'sgd_momentum': 0.8697323409248159}\n",
            "number of trainable parameter =  37\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  3] LR: 0.02526740  Train loss: 0.7245,  Val loss: 0.5224,  Test Acc: 0.6445\n",
            "[Epoch:   2/  3] LR: 0.02526740  Train loss: 0.6527,  Val loss: 0.5291,  Test Acc: 0.6631\n",
            "[Epoch:   3/  3] LR: 0.02526740  Train loss: 0.6256,  Val loss: 0.5789,  Test Acc: 0.6260\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.5363630698330327, 'fc1': 15, 'lr': 0.048983131340016604, 'num_layers': 1, 'optimizer': 'Adam'}\n",
            "number of trainable parameter =  61\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  3] LR: 0.04898313  Train loss: 0.7150,  Val loss: 0.5546,  Test Acc: 0.6465\n",
            "[Epoch:   2/  3] LR: 0.04898313  Train loss: 0.6413,  Val loss: 0.5814,  Test Acc: 0.6279\n",
            "[Epoch:   3/  3] LR: 0.04898313  Train loss: 0.6294,  Val loss: 0.6047,  Test Acc: 0.6279\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.2774114560464111, 'fc1': 16, 'lr': 0.06025447242961073, 'num_layers': 2, 'optimizer': 'Adam', 'fc2': 5}\n",
            "number of trainable parameter =  139\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  3] LR: 0.06025447  Train loss: 0.7347,  Val loss: 0.6242,  Test Acc: 0.6240\n",
            "[Epoch:   2/  3] LR: 0.06025447  Train loss: 0.6807,  Val loss: 0.6857,  Test Acc: 0.5127\n",
            "[Epoch:   3/  3] LR: 0.06025447  Train loss: 0.6902,  Val loss: 0.6724,  Test Acc: 0.5898\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.1363335612885128, 'fc1': 10, 'lr': 0.007822519784215385, 'num_layers': 1, 'optimizer': 'Adam'}\n",
            "number of trainable parameter =  41\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  3] LR: 0.00782252  Train loss: 0.7059,  Val loss: 0.6479,  Test Acc: 0.4922\n",
            "[Epoch:   2/  3] LR: 0.00782252  Train loss: 0.6467,  Val loss: 0.5988,  Test Acc: 0.6494\n",
            "[Epoch:   3/  3] LR: 0.00782252  Train loss: 0.6309,  Val loss: 0.5694,  Test Acc: 0.6445\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.23926990426296638, 'fc1': 12, 'lr': 0.08440803961148495, 'num_layers': 1, 'optimizer': 'Adam'}\n",
            "number of trainable parameter =  49\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  9] LR: 0.08440804  Train loss: 0.6906,  Val loss: 0.5245,  Test Acc: 0.6670\n",
            "[Epoch:   2/  9] LR: 0.08440804  Train loss: 0.6242,  Val loss: 0.5621,  Test Acc: 0.6396\n",
            "[Epoch:   3/  9] LR: 0.08440804  Train loss: 0.6173,  Val loss: 0.5507,  Test Acc: 0.6104\n",
            "[Epoch:   4/  9] LR: 0.08440804  Train loss: 0.5888,  Val loss: 0.5313,  Test Acc: 0.6533\n",
            "[Epoch:   5/  9] LR: 0.08440804  Train loss: 0.5806,  Val loss: 0.5084,  Test Acc: 0.6797\n",
            "[Epoch:   6/  9] LR: 0.08440804  Train loss: 0.5500,  Val loss: 0.4898,  Test Acc: 0.6475\n",
            "[Epoch:   7/  9] LR: 0.08440804  Train loss: 0.5341,  Val loss: 0.4848,  Test Acc: 0.7012\n",
            "[Epoch:   8/  9] LR: 0.08440804  Train loss: 0.5244,  Val loss: 0.4675,  Test Acc: 0.6787\n",
            "[Epoch:   9/  9] LR: 0.08440804  Train loss: 0.4823,  Val loss: 0.4698,  Test Acc: 0.7412\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.7704838709389494, 'fc1': 9, 'lr': 0.025267404740427272, 'num_layers': 1, 'optimizer': 'SGD', 'sgd_momentum': 0.8697323409248159}\n",
            "number of trainable parameter =  37\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  9] LR: 0.02526740  Train loss: 0.9260,  Val loss: 0.7909,  Test Acc: 0.4893\n",
            "[Epoch:   2/  9] LR: 0.02526740  Train loss: 0.7247,  Val loss: 0.6428,  Test Acc: 0.6006\n",
            "[Epoch:   3/  9] LR: 0.02526740  Train loss: 0.6423,  Val loss: 0.6032,  Test Acc: 0.5957\n",
            "[Epoch:   4/  9] LR: 0.02526740  Train loss: 0.6243,  Val loss: 0.6021,  Test Acc: 0.6221\n",
            "[Epoch:   5/  9] LR: 0.02526740  Train loss: 0.6298,  Val loss: 0.6081,  Test Acc: 0.6270\n",
            "[Epoch:   6/  9] LR: 0.02526740  Train loss: 0.6245,  Val loss: 0.5853,  Test Acc: 0.6357\n",
            "[Epoch:   7/  9] LR: 0.02526740  Train loss: 0.6248,  Val loss: 0.5697,  Test Acc: 0.6279\n",
            "[Epoch:   8/  9] LR: 0.02526740  Train loss: 0.6174,  Val loss: 0.5633,  Test Acc: 0.6396\n",
            "[Epoch:   9/  9] LR: 0.02526740  Train loss: 0.6116,  Val loss: 0.5607,  Test Acc: 0.6416\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.1363335612885128, 'fc1': 10, 'lr': 0.007822519784215385, 'num_layers': 1, 'optimizer': 'Adam'}\n",
            "number of trainable parameter =  41\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  9] LR: 0.00782252  Train loss: 0.7146,  Val loss: 0.6929,  Test Acc: 0.5430\n",
            "[Epoch:   2/  9] LR: 0.00782252  Train loss: 0.6507,  Val loss: 0.6323,  Test Acc: 0.5869\n",
            "[Epoch:   3/  9] LR: 0.00782252  Train loss: 0.6223,  Val loss: 0.6002,  Test Acc: 0.6045\n",
            "[Epoch:   4/  9] LR: 0.00782252  Train loss: 0.6168,  Val loss: 0.5804,  Test Acc: 0.6201\n",
            "[Epoch:   5/  9] LR: 0.00782252  Train loss: 0.6157,  Val loss: 0.5693,  Test Acc: 0.6279\n",
            "[Epoch:   6/  9] LR: 0.00782252  Train loss: 0.6167,  Val loss: 0.5612,  Test Acc: 0.6299\n",
            "[Epoch:   7/  9] LR: 0.00782252  Train loss: 0.6156,  Val loss: 0.5557,  Test Acc: 0.6387\n",
            "[Epoch:   8/  9] LR: 0.00782252  Train loss: 0.6123,  Val loss: 0.5547,  Test Acc: 0.6406\n",
            "[Epoch:   9/  9] LR: 0.00782252  Train loss: 0.6101,  Val loss: 0.5542,  Test Acc: 0.6387\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.23926990426296638, 'fc1': 12, 'lr': 0.08440803961148495, 'num_layers': 1, 'optimizer': 'Adam'}\n",
            "number of trainable parameter =  49\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/ 27] LR: 0.08440804  Train loss: 0.8960,  Val loss: 0.6450,  Test Acc: 0.5957\n",
            "[Epoch:   2/ 27] LR: 0.08440804  Train loss: 0.6256,  Val loss: 0.5193,  Test Acc: 0.6826\n",
            "[Epoch:   3/ 27] LR: 0.08440804  Train loss: 0.6453,  Val loss: 0.5107,  Test Acc: 0.6875\n",
            "[Epoch:   4/ 27] LR: 0.08440804  Train loss: 0.5972,  Val loss: 0.5161,  Test Acc: 0.6748\n",
            "[Epoch:   5/ 27] LR: 0.08440804  Train loss: 0.5802,  Val loss: 0.5441,  Test Acc: 0.6289\n",
            "[Epoch:   6/ 27] LR: 0.08440804  Train loss: 0.5658,  Val loss: 0.5524,  Test Acc: 0.6592\n",
            "[Epoch:   7/ 27] LR: 0.08440804  Train loss: 0.5419,  Val loss: 0.5119,  Test Acc: 0.7568\n",
            "[Epoch:   8/ 27] LR: 0.08440804  Train loss: 0.5125,  Val loss: 0.4813,  Test Acc: 0.7207\n",
            "[Epoch:   9/ 27] LR: 0.08440804  Train loss: 0.4786,  Val loss: 0.4463,  Test Acc: 0.7295\n",
            "[Epoch:  10/ 27] LR: 0.08440804  Train loss: 0.4629,  Val loss: 0.4595,  Test Acc: 0.7139\n",
            "[Epoch:  11/ 27] LR: 0.08440804  Train loss: 0.4401,  Val loss: 0.4284,  Test Acc: 0.7471\n",
            "[Epoch:  12/ 27] LR: 0.08440804  Train loss: 0.4108,  Val loss: 0.4254,  Test Acc: 0.7441\n",
            "[Epoch:  13/ 27] LR: 0.08440804  Train loss: 0.4012,  Val loss: 0.4000,  Test Acc: 0.8213\n",
            "[Epoch:  14/ 27] LR: 0.08440804  Train loss: 0.3945,  Val loss: 0.3805,  Test Acc: 0.8467\n",
            "[Epoch:  15/ 27] LR: 0.08440804  Train loss: 0.3610,  Val loss: 0.3927,  Test Acc: 0.7900\n",
            "[Epoch:  16/ 27] LR: 0.08440804  Train loss: 0.3441,  Val loss: 0.3092,  Test Acc: 0.8438\n",
            "[Epoch:  17/ 27] LR: 0.08440804  Train loss: 0.3433,  Val loss: 0.3308,  Test Acc: 0.8887\n",
            "[Epoch:  18/ 27] LR: 0.08440804  Train loss: 0.3243,  Val loss: 0.3530,  Test Acc: 0.8457\n",
            "[Epoch:  19/ 27] LR: 0.08440804  Train loss: 0.3170,  Val loss: 0.2798,  Test Acc: 0.8711\n",
            "[Epoch:  20/ 27] LR: 0.08440804  Train loss: 0.3158,  Val loss: 0.3087,  Test Acc: 0.8525\n",
            "[Epoch:  21/ 27] LR: 0.08440804  Train loss: 0.2944,  Val loss: 0.3337,  Test Acc: 0.8623\n",
            "[Epoch:  22/ 27] LR: 0.08440804  Train loss: 0.2911,  Val loss: 0.2718,  Test Acc: 0.9033\n",
            "[Epoch:  23/ 27] LR: 0.08440804  Train loss: 0.2798,  Val loss: 0.2902,  Test Acc: 0.8740\n",
            "[Epoch:  24/ 27] LR: 0.08440804  Train loss: 0.2660,  Val loss: 0.2922,  Test Acc: 0.9033\n",
            "[Epoch:  25/ 27] LR: 0.08440804  Train loss: 0.2639,  Val loss: 0.2782,  Test Acc: 0.9082\n",
            "[Epoch:  26/ 27] LR: 0.08440804  Train loss: 0.2476,  Val loss: 0.2827,  Test Acc: 0.8867\n",
            "[Epoch:  27/ 27] LR: 0.08440804  Train loss: 0.2537,  Val loss: 0.2723,  Test Acc: 0.8896\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.19729213840218665, 'fc1': 6, 'lr': 0.09675442315335843, 'num_layers': 1, 'optimizer': 'SGD', 'sgd_momentum': 0.8729839373087562}\n",
            "number of trainable parameter =  25\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  9] LR: 0.09675442  Train loss: 0.8073,  Val loss: 0.5940,  Test Acc: 0.6201\n",
            "[Epoch:   2/  9] LR: 0.09675442  Train loss: 0.6591,  Val loss: 0.6000,  Test Acc: 0.6182\n",
            "[Epoch:   3/  9] LR: 0.09675442  Train loss: 0.6673,  Val loss: 0.6191,  Test Acc: 0.5996\n",
            "[Epoch:   4/  9] LR: 0.09675442  Train loss: 0.6434,  Val loss: 0.5340,  Test Acc: 0.6719\n",
            "[Epoch:   5/  9] LR: 0.09675442  Train loss: 0.6314,  Val loss: 0.5615,  Test Acc: 0.6562\n",
            "[Epoch:   6/  9] LR: 0.09675442  Train loss: 0.6171,  Val loss: 0.5818,  Test Acc: 0.6201\n",
            "[Epoch:   7/  9] LR: 0.09675442  Train loss: 0.6176,  Val loss: 0.5562,  Test Acc: 0.6436\n",
            "[Epoch:   8/  9] LR: 0.09675442  Train loss: 0.6056,  Val loss: 0.5323,  Test Acc: 0.6602\n",
            "[Epoch:   9/  9] LR: 0.09675442  Train loss: 0.6124,  Val loss: 0.5444,  Test Acc: 0.6660\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.8629506419939672, 'fc1': 18, 'lr': 0.05370046333397184, 'num_layers': 2, 'optimizer': 'SGD', 'fc2': 20, 'sgd_momentum': 0.892099524931147}\n",
            "number of trainable parameter =  455\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  9] LR: 0.05370046  Train loss: 0.7982,  Val loss: 0.6572,  Test Acc: 0.5596\n",
            "[Epoch:   2/  9] LR: 0.05370046  Train loss: 0.6968,  Val loss: 0.6168,  Test Acc: 0.6113\n",
            "[Epoch:   3/  9] LR: 0.05370046  Train loss: 0.6702,  Val loss: 0.6112,  Test Acc: 0.5791\n",
            "[Epoch:   4/  9] LR: 0.05370046  Train loss: 0.6811,  Val loss: 0.5998,  Test Acc: 0.6211\n",
            "[Epoch:   5/  9] LR: 0.05370046  Train loss: 0.7412,  Val loss: 0.6102,  Test Acc: 0.6191\n",
            "[Epoch:   6/  9] LR: 0.05370046  Train loss: 0.6929,  Val loss: 0.6190,  Test Acc: 0.6250\n",
            "[Epoch:   7/  9] LR: 0.05370046  Train loss: 0.7251,  Val loss: 0.6282,  Test Acc: 0.6338\n",
            "[Epoch:   8/  9] LR: 0.05370046  Train loss: 0.6634,  Val loss: 0.6109,  Test Acc: 0.6387\n",
            "[Epoch:   9/  9] LR: 0.05370046  Train loss: 0.6704,  Val loss: 0.6012,  Test Acc: 0.6270\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.7579831205610741, 'fc1': 18, 'lr': 0.004499052759905523, 'num_layers': 3, 'optimizer': 'Adam', 'fc2': 17, 'fc3': 9}\n",
            "number of trainable parameter =  395\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  9] LR: 0.00449905  Train loss: 0.8543,  Val loss: 0.7366,  Test Acc: 0.4473\n",
            "[Epoch:   2/  9] LR: 0.00449905  Train loss: 0.7298,  Val loss: 0.6865,  Test Acc: 0.4209\n",
            "[Epoch:   3/  9] LR: 0.00449905  Train loss: 0.7934,  Val loss: 0.6501,  Test Acc: 0.6377\n",
            "[Epoch:   4/  9] LR: 0.00449905  Train loss: 0.6777,  Val loss: 0.6293,  Test Acc: 0.6387\n",
            "[Epoch:   5/  9] LR: 0.00449905  Train loss: 0.7185,  Val loss: 0.6206,  Test Acc: 0.6250\n",
            "[Epoch:   6/  9] LR: 0.00449905  Train loss: 0.6716,  Val loss: 0.6151,  Test Acc: 0.6270\n",
            "[Epoch:   7/  9] LR: 0.00449905  Train loss: 0.6925,  Val loss: 0.6113,  Test Acc: 0.6309\n",
            "[Epoch:   8/  9] LR: 0.00449905  Train loss: 0.6535,  Val loss: 0.6052,  Test Acc: 0.6406\n",
            "[Epoch:   9/  9] LR: 0.00449905  Train loss: 0.6431,  Val loss: 0.6001,  Test Acc: 0.6436\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.21651449504940304, 'fc1': 12, 'lr': 0.0013644234356103852, 'num_layers': 2, 'optimizer': 'Adam', 'fc2': 17}\n",
            "number of trainable parameter =  275\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  9] LR: 0.00136442  Train loss: 0.7623,  Val loss: 0.7025,  Test Acc: 0.4795\n",
            "[Epoch:   2/  9] LR: 0.00136442  Train loss: 0.7315,  Val loss: 0.6732,  Test Acc: 0.5020\n",
            "[Epoch:   3/  9] LR: 0.00136442  Train loss: 0.6972,  Val loss: 0.6509,  Test Acc: 0.5332\n",
            "[Epoch:   4/  9] LR: 0.00136442  Train loss: 0.6953,  Val loss: 0.6311,  Test Acc: 0.5830\n",
            "[Epoch:   5/  9] LR: 0.00136442  Train loss: 0.6752,  Val loss: 0.6148,  Test Acc: 0.6357\n",
            "[Epoch:   6/  9] LR: 0.00136442  Train loss: 0.6463,  Val loss: 0.6025,  Test Acc: 0.6553\n",
            "[Epoch:   7/  9] LR: 0.00136442  Train loss: 0.6670,  Val loss: 0.5935,  Test Acc: 0.6660\n",
            "[Epoch:   8/  9] LR: 0.00136442  Train loss: 0.6414,  Val loss: 0.5860,  Test Acc: 0.6719\n",
            "[Epoch:   9/  9] LR: 0.00136442  Train loss: 0.6455,  Val loss: 0.5793,  Test Acc: 0.6719\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.39555276471441875, 'fc1': 12, 'lr': 0.09671283362426662, 'num_layers': 2, 'optimizer': 'Adam', 'fc2': 19}\n",
            "number of trainable parameter =  303\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  9] LR: 0.09671283  Train loss: 0.7016,  Val loss: 0.6043,  Test Acc: 0.6816\n",
            "[Epoch:   2/  9] LR: 0.09671283  Train loss: 0.6748,  Val loss: 0.6312,  Test Acc: 0.5840\n",
            "[Epoch:   3/  9] LR: 0.09671283  Train loss: 0.6500,  Val loss: 0.5691,  Test Acc: 0.6348\n",
            "[Epoch:   4/  9] LR: 0.09671283  Train loss: 0.6288,  Val loss: 0.5355,  Test Acc: 0.6514\n",
            "[Epoch:   5/  9] LR: 0.09671283  Train loss: 0.6055,  Val loss: 0.5314,  Test Acc: 0.6357\n",
            "[Epoch:   6/  9] LR: 0.09671283  Train loss: 0.5917,  Val loss: 0.5100,  Test Acc: 0.6387\n",
            "[Epoch:   7/  9] LR: 0.09671283  Train loss: 0.5908,  Val loss: 0.5005,  Test Acc: 0.7246\n",
            "[Epoch:   8/  9] LR: 0.09671283  Train loss: 0.5443,  Val loss: 0.4262,  Test Acc: 0.7080\n",
            "[Epoch:   9/  9] LR: 0.09671283  Train loss: 0.4948,  Val loss: 0.3792,  Test Acc: 0.7041\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.8449032482245846, 'fc1': 2, 'lr': 0.0017715852345078008, 'num_layers': 3, 'optimizer': 'SGD', 'fc2': 13, 'fc3': 7, 'sgd_momentum': 0.7590475493632811}\n",
            "number of trainable parameter =  59\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/  9] LR: 0.00177159  Train loss: 0.7176,  Val loss: 0.6842,  Test Acc: 0.5752\n",
            "[Epoch:   2/  9] LR: 0.00177159  Train loss: 0.7085,  Val loss: 0.6846,  Test Acc: 0.5781\n",
            "[Epoch:   3/  9] LR: 0.00177159  Train loss: 0.7246,  Val loss: 0.6856,  Test Acc: 0.5723\n",
            "[Epoch:   4/  9] LR: 0.00177159  Train loss: 0.7057,  Val loss: 0.6862,  Test Acc: 0.5752\n",
            "[Epoch:   5/  9] LR: 0.00177159  Train loss: 0.6894,  Val loss: 0.6866,  Test Acc: 0.5771\n",
            "[Epoch:   6/  9] LR: 0.00177159  Train loss: 0.6916,  Val loss: 0.6867,  Test Acc: 0.5811\n",
            "[Epoch:   7/  9] LR: 0.00177159  Train loss: 0.7194,  Val loss: 0.6865,  Test Acc: 0.5801\n",
            "[Epoch:   8/  9] LR: 0.00177159  Train loss: 0.7163,  Val loss: 0.6869,  Test Acc: 0.5840\n",
            "[Epoch:   9/  9] LR: 0.00177159  Train loss: 0.7037,  Val loss: 0.6870,  Test Acc: 0.5879\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.19729213840218665, 'fc1': 6, 'lr': 0.09675442315335843, 'num_layers': 1, 'optimizer': 'SGD', 'sgd_momentum': 0.8729839373087562}\n",
            "number of trainable parameter =  25\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/ 27] LR: 0.09675442  Train loss: 0.6712,  Val loss: 0.6480,  Test Acc: 0.5391\n",
            "[Epoch:   2/ 27] LR: 0.09675442  Train loss: 0.6363,  Val loss: 0.5833,  Test Acc: 0.6396\n",
            "[Epoch:   3/ 27] LR: 0.09675442  Train loss: 0.6203,  Val loss: 0.5766,  Test Acc: 0.6338\n",
            "[Epoch:   4/ 27] LR: 0.09675442  Train loss: 0.6257,  Val loss: 0.5676,  Test Acc: 0.6367\n",
            "[Epoch:   5/ 27] LR: 0.09675442  Train loss: 0.6305,  Val loss: 0.5608,  Test Acc: 0.6445\n",
            "[Epoch:   6/ 27] LR: 0.09675442  Train loss: 0.6163,  Val loss: 0.5611,  Test Acc: 0.6338\n",
            "[Epoch:   7/ 27] LR: 0.09675442  Train loss: 0.6195,  Val loss: 0.5743,  Test Acc: 0.6230\n",
            "[Epoch:   8/ 27] LR: 0.09675442  Train loss: 0.6150,  Val loss: 0.5823,  Test Acc: 0.6270\n",
            "[Epoch:   9/ 27] LR: 0.09675442  Train loss: 0.6065,  Val loss: 0.5587,  Test Acc: 0.6172\n",
            "[Epoch:  10/ 27] LR: 0.09675442  Train loss: 0.6073,  Val loss: 0.5367,  Test Acc: 0.6475\n",
            "[Epoch:  11/ 27] LR: 0.09675442  Train loss: 0.6073,  Val loss: 0.5435,  Test Acc: 0.6318\n",
            "[Epoch:  12/ 27] LR: 0.09675442  Train loss: 0.5969,  Val loss: 0.5246,  Test Acc: 0.6357\n",
            "[Epoch:  13/ 27] LR: 0.09675442  Train loss: 0.5943,  Val loss: 0.5263,  Test Acc: 0.6367\n",
            "[Epoch:  14/ 27] LR: 0.09675442  Train loss: 0.5952,  Val loss: 0.5557,  Test Acc: 0.6084\n",
            "[Epoch:  15/ 27] LR: 0.09675442  Train loss: 0.5914,  Val loss: 0.5341,  Test Acc: 0.6504\n",
            "[Epoch:  16/ 27] LR: 0.09675442  Train loss: 0.5808,  Val loss: 0.5287,  Test Acc: 0.6475\n",
            "[Epoch:  17/ 27] LR: 0.09675442  Train loss: 0.5735,  Val loss: 0.5128,  Test Acc: 0.6533\n",
            "[Epoch:  18/ 27] LR: 0.09675442  Train loss: 0.5799,  Val loss: 0.5066,  Test Acc: 0.6553\n",
            "[Epoch:  19/ 27] LR: 0.09675442  Train loss: 0.5726,  Val loss: 0.5123,  Test Acc: 0.6602\n",
            "[Epoch:  20/ 27] LR: 0.09675442  Train loss: 0.5627,  Val loss: 0.5037,  Test Acc: 0.6592\n",
            "[Epoch:  21/ 27] LR: 0.09675442  Train loss: 0.5579,  Val loss: 0.5237,  Test Acc: 0.6777\n",
            "[Epoch:  22/ 27] LR: 0.09675442  Train loss: 0.5573,  Val loss: 0.4874,  Test Acc: 0.6895\n",
            "[Epoch:  23/ 27] LR: 0.09675442  Train loss: 0.5448,  Val loss: 0.5250,  Test Acc: 0.6631\n",
            "[Epoch:  24/ 27] LR: 0.09675442  Train loss: 0.5327,  Val loss: 0.4706,  Test Acc: 0.6992\n",
            "[Epoch:  25/ 27] LR: 0.09675442  Train loss: 0.5242,  Val loss: 0.4733,  Test Acc: 0.7559\n",
            "[Epoch:  26/ 27] LR: 0.09675442  Train loss: 0.5191,  Val loss: 0.4919,  Test Acc: 0.7236\n",
            "[Epoch:  27/ 27] LR: 0.09675442  Train loss: 0.5104,  Val loss: 0.4774,  Test Acc: 0.7207\n",
            "Training is terminated. final epoch or validation loss has increased\n",
            "Selected HyperParameters to test: \n",
            "{'dropout': 0.39555276471441875, 'fc1': 12, 'lr': 0.09671283362426662, 'num_layers': 2, 'optimizer': 'Adam', 'fc2': 19}\n",
            "number of trainable parameter =  303\n",
            "loading training, val and test set completed!\n",
            "[Epoch:   1/ 27] LR: 0.09671283  Train loss: 0.7367,  Val loss: 0.6267,  Test Acc: 0.5801\n",
            "[Epoch:   2/ 27] LR: 0.09671283  Train loss: 0.6747,  Val loss: 0.5722,  Test Acc: 0.6328\n",
            "[Epoch:   3/ 27] LR: 0.09671283  Train loss: 0.6350,  Val loss: 0.5895,  Test Acc: 0.6426\n",
            "[Epoch:   4/ 27] LR: 0.09671283  Train loss: 0.6420,  Val loss: 0.5423,  Test Acc: 0.6338\n",
            "[Epoch:   5/ 27] LR: 0.09671283  Train loss: 0.6186,  Val loss: 0.5513,  Test Acc: 0.6230\n",
            "[Epoch:   6/ 27] LR: 0.09671283  Train loss: 0.6339,  Val loss: 0.4498,  Test Acc: 0.6377\n",
            "[Epoch:   7/ 27] LR: 0.09671283  Train loss: 0.6276,  Val loss: 0.5881,  Test Acc: 0.6924\n",
            "[Epoch:   8/ 27] LR: 0.09671283  Train loss: 0.6045,  Val loss: 0.4609,  Test Acc: 0.6621\n",
            "[Epoch:   9/ 27] LR: 0.09671283  Train loss: 0.5274,  Val loss: 0.4049,  Test Acc: 0.6328\n",
            "[Epoch:  10/ 27] LR: 0.09671283  Train loss: 0.5312,  Val loss: 0.4374,  Test Acc: 0.7568\n",
            "[Epoch:  11/ 27] LR: 0.09671283  Train loss: 0.4928,  Val loss: 0.3976,  Test Acc: 0.7578\n",
            "[Epoch:  12/ 27] LR: 0.09671283  Train loss: 0.4533,  Val loss: 0.3508,  Test Acc: 0.7617\n",
            "[Epoch:  13/ 27] LR: 0.09671283  Train loss: 0.4095,  Val loss: 0.4011,  Test Acc: 0.7559\n",
            "[Epoch:  14/ 27] LR: 0.09671283  Train loss: 0.4771,  Val loss: 0.3399,  Test Acc: 0.7627\n",
            "[Epoch:  15/ 27] LR: 0.09671283  Train loss: 0.5194,  Val loss: 0.3685,  Test Acc: 0.7275\n",
            "[Epoch:  16/ 27] LR: 0.09671283  Train loss: 0.4854,  Val loss: 0.3838,  Test Acc: 0.7168\n",
            "[Epoch:  17/ 27] LR: 0.09671283  Train loss: 0.4548,  Val loss: 0.3307,  Test Acc: 0.7197\n",
            "[Epoch:  18/ 27] LR: 0.09671283  Train loss: 0.4537,  Val loss: 0.3250,  Test Acc: 0.7471\n",
            "[Epoch:  19/ 27] LR: 0.09671283  Train loss: 0.4414,  Val loss: 0.3353,  Test Acc: 0.7344\n",
            "[Epoch:  20/ 27] LR: 0.09671283  Train loss: 0.4226,  Val loss: 0.3283,  Test Acc: 0.7646\n",
            "[Epoch:  21/ 27] LR: 0.09671283  Train loss: 0.4474,  Val loss: 0.3161,  Test Acc: 0.7471\n",
            "[Epoch:  22/ 27] LR: 0.09671283  Train loss: 0.4326,  Val loss: 0.3206,  Test Acc: 0.7080\n",
            "[Epoch:  23/ 27] LR: 0.09671283  Train loss: 0.4233,  Val loss: 0.3419,  Test Acc: 0.7373\n",
            "[Epoch:  24/ 27] LR: 0.09671283  Train loss: 0.4277,  Val loss: 0.3252,  Test Acc: 0.7451\n",
            "[Epoch:  25/ 27] LR: 0.09671283  Train loss: 0.4370,  Val loss: 0.3046,  Test Acc: 0.7607\n",
            "[Epoch:  26/ 27] LR: 0.09671283  Train loss: 0.4333,  Val loss: 0.2682,  Test Acc: 0.7705\n",
            "[Epoch:  27/ 27] LR: 0.09671283  Train loss: 0.4326,  Val loss: 0.2837,  Test Acc: 0.7666\n",
            "Training is terminated. final epoch or validation loss has increased\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ga13KQCx9zh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "outputId": "f3f73471-c6e5-4e98-e2e5-4e2ef666f8c5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import hpbandster.core.result as hpres\n",
        "import hpbandster.visualization as hpvis\n",
        "\n",
        "# load the example run from the log files\n",
        "result = hpres.logged_results_to_HBS_result('.')\n",
        "\n",
        "# get all executed runs\n",
        "all_runs = result.get_all_runs()\n",
        "\n",
        "# get the 'dict' that translates config ids to the actual configurations\n",
        "id2conf = result.get_id2config_mapping()\n",
        "\n",
        "\n",
        "# Here is how you get he incumbent (best configuration)\n",
        "inc_id = result.get_incumbent_id()\n",
        "\n",
        "# let's grab the run on the highest budget\n",
        "inc_runs = result.get_runs_by_id(inc_id)\n",
        "inc_run = inc_runs[-1]\n",
        "\n",
        "\n",
        "# We have access to all information: the config, the loss observed during\n",
        "#optimization, and all the additional information\n",
        "inc_loss = inc_run.loss\n",
        "inc_config = id2conf[inc_id]['config']\n",
        "inc_test_loss = inc_run.info['test accuracy']\n",
        "\n",
        "print('Best found configuration:')\n",
        "print(inc_config)\n",
        "print('It achieved validation loss of %f and test accuracy of %f .'%(inc_loss, inc_test_loss))\n",
        "\n",
        "\n",
        "# Let's plot the observed losses grouped by budget,\n",
        "hpvis.losses_over_time(all_runs)\n",
        "\n",
        "# and the number of finished runs.\n",
        "hpvis.finished_runs_over_time(all_runs)\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best found configuration:\n",
            "{'dropout': 0.23926990426296638, 'fc1': 12, 'lr': 0.08440803961148495, 'num_layers': 1, 'optimizer': 'Adam'}\n",
            "It achieved validation loss of 0.272315 and test accuracy of 0.889648 .\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}